{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## While evaluating (testing) your algorithms, you should not consider images of your current query identity taken from the same camera. For example, when you create ranklist for the first query image (index 22, label 3, camera 1, name \"1_003_1_02.png\"), you should not include images with indexes 21, 23, 24 in this ranking list, as these are images of the same person (label 3) captured by the camera with index 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "#contains indexes of images that can be used for training and validation (Training Validating)   (7368,) 7368 images  (7-10) per person\n",
    "train_idxs = loadmat('./PR_data/cuhk03_new_protocol_config_labeled.mat')['train_idx'].flatten()\n",
    "\n",
    "#specifies whether image was taken from camera 1 or camera 2.    \n",
    "camId = loadmat('./PR_data/cuhk03_new_protocol_config_labeled.mat')['camId'].flatten()\n",
    "\n",
    "# specifies correspondences between names of files in images_cuhk03 and their indexes\n",
    "filelist = loadmat('./PR_data/cuhk03_new_protocol_config_labeled.mat')['filelist'].flatten()\n",
    "\n",
    "#specifies indexes of the part of the dataset from which you compose your ranklists during testing phase (Testing)\n",
    "gallery_idx = loadmat('./PR_data/cuhk03_new_protocol_config_labeled.mat')['gallery_idx'].flatten()\n",
    "\n",
    "#contains ground truths for each image                                                           (14096,) 对应着train_idxs的image label\n",
    "labels = loadmat('./PR_data/cuhk03_new_protocol_config_labeled.mat')['labels'].flatten()\n",
    "\n",
    "#contains indexes of query images (Testing)\n",
    "query_idx = loadmat('./PR_data/cuhk03_new_protocol_config_labeled.mat')['query_idx'].flatten()\n",
    "\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7368,)\n"
     ]
    }
   ],
   "source": [
    "print(train_idxs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import JSON file (14096 x 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "with open('feature_data.json', 'r') as f:\n",
    "    features = json.load(f)\n",
    "    \n",
    "feature = np.asarray(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Testing & Validating set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train = []\n",
    "lbl_train = []\n",
    "\n",
    "for i in train_idxs:\n",
    "    fea_train.append(feature[i-1])\n",
    "    lbl_train.append(labels[i-1])\n",
    "\n",
    "fea_train = np.asarray(fea_train)\n",
    "lbl_train = np.asarray(lbl_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_feature = []\n",
    "query_lbl = []\n",
    "query_camid = []\n",
    "\n",
    "for i in query_idx:\n",
    "    query_feature.append(feature[i-1])\n",
    "    query_lbl.append(labels[i-1])\n",
    "    query_camid.append(camId[i-1])\n",
    "\n",
    "query_feature = np.asarray(query_feature)\n",
    "query_lbl = np.asarray(query_lbl)\n",
    "query_camid = np.asarray(query_camid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery_feature = []\n",
    "gallery_lbl = []\n",
    "gallery_camid = []\n",
    "\n",
    "for i in gallery_idx:\n",
    "    gallery_feature.append(feature[i-1])\n",
    "    gallery_lbl.append(labels[i-1])\n",
    "    gallery_camid.append(camId[i-1])\n",
    "\n",
    "gallery_feature = np.asarray(gallery_feature)\n",
    "gallery_lbl = np.asarray(gallery_lbl)\n",
    "gallery_camid = np.asarray(gallery_camid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine lbl and feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = query_feature.T\n",
    "\n",
    "#feature, camid, lbl\n",
    "#0:2047 2048     2049\n",
    "q_combine = (np.vstack((q,query_camid,query_lbl))).T\n",
    "\n",
    "#feature, camid, lbl\n",
    "g = gallery_feature.T\n",
    "g_combine = (np.vstack( ( g, gallery_camid, gallery_lbl ))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 1400)\n",
      "(1400,)\n",
      "(1400,)\n",
      "(2050, 1400)\n",
      "(2048, 5328)\n",
      "(5328,)\n",
      "(5328,)\n",
      "(5328, 2050)\n"
     ]
    }
   ],
   "source": [
    "print(q.shape)\n",
    "print(query_camid.shape)\n",
    "print(query_lbl.shape)\n",
    "print(q_combine.T.shape)\n",
    "\n",
    "print(g.shape)\n",
    "print(gallery_camid.shape)\n",
    "print(gallery_lbl.shape)\n",
    "print(g_combine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Augmented: (1400, 2050)\n",
      "Gallery Augmented: (5328, 2050)\n",
      "[0.09531084 0.35792962 0.1479917  ... 0.1575031  1.         6.        ]\n",
      "[0.82696682 0.39545193 0.13419056 ... 0.03505545 1.         3.        ]\n"
     ]
    }
   ],
   "source": [
    "print( 'Query Augmented: {}'.format( q_combine.shape ) )\n",
    "print( 'Gallery Augmented: {}'.format( g_combine.shape ) )\n",
    "print(q_combine[2])\n",
    "print(g_combine[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1400/1400 [00:23<00:00, 58.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "\n",
    "X_train = g_combine[:,:-2]\n",
    "y_train = g_combine[:,-1]\n",
    "classifier = NearestNeighbors(n_neighbors=20, metric = 'euclidean')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "query_rank_list_20 = []\n",
    "\n",
    "acc = []\n",
    "\n",
    "for i in tqdm(range(q_combine.shape[0])):\n",
    "    \n",
    "    query_lbl = q_combine[i,-1].astype(int)\n",
    "    \n",
    "    \n",
    "    X_test = q_combine[i,:-2].reshape(1,-1)\n",
    "    dist, index = classifier.kneighbors(X_test)\n",
    "    index = index.flatten()\n",
    "    ii = 0\n",
    "    rank_list = []\n",
    "    for j in index:\n",
    "        if len(rank_list) < 15:\n",
    "            if g_combine[j,-1] !=  q_combine[i,-1] or g_combine[j,-2] !=  q_combine[i,-2]:\n",
    "                rank_list.append(g_combine[j,-1].astype(int) == query_lbl)\n",
    "    query_rank_list_20.append(rank_list)\n",
    "\n",
    "query_rank_list_20 = np.asarray(query_rank_list_20)\n",
    "np.savetxt( 'baseline_ranklist.csv', query_rank_list_20, delimiter= ',' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 1: 47.0%\n",
      "rank 5: 66.85714285714286%\n",
      "rank 10: 74.92857142857143%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program_user\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Program_user\\Anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Program_user\\Anaconda\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP :0.4988458535679765\n"
     ]
    }
   ],
   "source": [
    "rank1 = query_rank_list_20.T[0].T\n",
    "rank5  = query_rank_list_20.T[:5].T\n",
    "rank10 = query_rank_list_20.T[:10].T\n",
    "\n",
    "cmc1  = rank1\n",
    "cmc5  = np.sum(rank5, axis = 1) > 0\n",
    "cmc10 = np.sum(rank10, axis = 1) > 0\n",
    "\n",
    "print( 'rank 1: {}%'.format(np.sum(cmc1)/cmc1.shape[0]* 100))\n",
    "print( 'rank 5: {}%'.format(np.sum(cmc5)/cmc5.shape[0]* 100 ))\n",
    "print( 'rank 10: {}%'.format(np.sum(cmc10)/cmc10.shape[0]*100))\n",
    "\n",
    "mAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mAP():\n",
    "    average_precision=[]\n",
    "    ranklist = np.loadtxt(open(\"baseline_ranklist.csv\", \"rb\"), delimiter=\",\")\n",
    "    for i in range(len(ranklist)):\n",
    "        precision=[]\n",
    "        recall=[]\n",
    "        precisions=[]\n",
    "        s=sum(ranklist[i,:])\n",
    "        for j in range(1,ranklist.shape[1]):\n",
    "            precision.append(sum(ranklist[i,:j]/j))\n",
    "            recall.append(sum(ranklist[i,:j]/s))\n",
    "            if recall[j-1] == 1:\n",
    "                  break\n",
    "\n",
    "        u=[]\n",
    "        indices=[]\n",
    "        recall=np.array(recall)\n",
    "        precision=np.array(precision)\n",
    "        u, indices=np.unique(recall,return_index=True)\n",
    "\n",
    "        precisions=precision[indices]\n",
    "        precisions=precisions[precisions!=0]\n",
    "        average_precision.append(np.mean(precisions))\n",
    "    average_precision = np.nan_to_num(average_precision)\n",
    "    print('mAP :{}'.format(np.mean(average_precision[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
